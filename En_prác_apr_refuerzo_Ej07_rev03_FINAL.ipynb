{"cells":[{"cell_type":"code","execution_count":1,"id":"3087bdd2-d6f2-47b0-8031-0bc934ff5e39","metadata":{"id":"3087bdd2-d6f2-47b0-8031-0bc934ff5e39","executionInfo":{"status":"ok","timestamp":1684217664343,"user_tz":-180,"elapsed":4,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"}}},"outputs":[],"source":["# LO HE HECHO EN COLAB\n","\n","# La práctica deberá realizarse de manera individual y debe entregarse en la plataforma de Instituto BME.\n","\n","# Para cada ejercicio: 1) Entregad un fichero en ipython notebook (.ipynb) ejecutable en Colab, y \n","# 2) cada notebook ha de incluir un comentario sobre el ejercicio y sus resultados.\n","\n","# 7) Utiliza el modelo de Q-learning funcional con distintas redes neuronales para resolver el entorno de RL-Bolsa.\n","# CONCLUSION: HE INTENTADO DE TODO, COMO SE VE MAS ABAJO PERO NO ME SALE."]},{"cell_type":"code","execution_count":2,"id":"0c20e51f-3b60-46da-9e39-5a363e54d9c3","metadata":{"id":"0c20e51f-3b60-46da-9e39-5a363e54d9c3","executionInfo":{"status":"ok","timestamp":1684217664344,"user_tz":-180,"elapsed":4,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"}}},"outputs":[],"source":["# tengo en los apuntes que Valero dijo en clase sobre este punto que se podia usar\n","# los modelos usados en el ejercicio 5\n","# para resolver este entorno.\n","\n","# el tema es que valores de discount_factor, alpha, y epsilon como inicialaes y \n","# hacer lo mismo que en el ejercio 4 para ver unos pre-optimos"]},{"cell_type":"code","execution_count":9,"id":"6b43eb84-1214-4b6c-9e3f-3bcb5f6c973b","metadata":{"executionInfo":{"elapsed":261,"status":"ok","timestamp":1684217791796,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"6b43eb84-1214-4b6c-9e3f-3bcb5f6c973b"},"outputs":[],"source":["#1 Copio el modelo del ejercicio 6\n","\n","# Libraries\n","# Basics\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","\n","# Reinforcement Learning\n","import gym\n","from gym import spaces\n","\n","# DATA\n","# Import data\n","data = pd.read_csv(\"googl.us.txt\", delimiter=\",\")\n","\n","# d. Incluir ruido en los datos.\n","# considero que meter ruido en los datos es que \n","# algunos datos no se tenian y entocnes se rellenaban con los del dia anterior\n","data = data['Open']\n","# viejo\n","#Genero en y un 10% de datos a 0\n","cuantos_random = int(data.shape[0]*0.1)\n","a_acero_array = []\n","for i in range(0,cuantos_random):\n","    a_cero = np.random.randint(0, data.shape[0])\n","    a_acero_array.append(a_cero)\n","data[a_acero_array] = np.nan\n","# pd.DataFrame(data).plot(figsize=(20,10))\n","data.fillna(method='ffill', inplace=True)\n","data.fillna(method='bfill', inplace=True)\n","# pd.DataFrame(data).plot(figsize=(20,10))\n","\n","# vector_incertidumbre = np.full(shape = y.shape[0], fill_value = 100, dtype=None)\n","# vector_incertidumbre[a_acero_array] = 0\n","# pd.DataFrame(vector_incertidumbre).plot(figsize=(20,10))\n","# y, y.shape, vector_incertidumbre, vector_incertidumbre.shape\n","\n","# DEFINITION OF ENVIRONMENT\n","# original de los notebook de Valero con Porcentaje comision\n","class OneStockEnv(gym.Env):\n","    '''\n","    Reinforcement Leaning environment representing a Stock Market with a single stock.\n","    '''\n","    def __init__(self, \n","                 data,\n","                 time_skip    = 1,\n","                 time_horizon = 10,\n","                 # comision = 0 #a.  Meter comisiones. Porcentqaje comision\n","                 porcentaje_comision = 0.0007 #a.  Meter comisiones. Porcentqaje comision\n","                 ):\n","        '''\n","        Initialization of the environment.\n","        \n","        Args:\n","            data\n","            time_skip    (int):      number of timesteps to skip between states\n","        '''\n","        # Get the data\n","        self.data = data\n","        self.time_horizon = time_horizon\n","        self.time_skip = time_skip\n","        #self.comision = comision #a.  Meter comisiones. Porcentqaje comision\n","        self.porcentaje_comision = porcentaje_comision #a.  Meter comisiones. Porcentqaje comision\n","        # Define actions\n","        self.actions = {0 : 'Hold', 1 : 'Buy', 2 : 'Sell'}\n","        \n","        # Save useful values\n","        self.n_timesteps = (self.data.shape[0] - self.time_horizon) \n","        self.n_features  = 1\n","        self.n_actions   = len(self.actions)\n","        self.state_shape = (self.time_horizon, self.n_features)\n","        \n","        # Define spaces\n","        self.action_space      = spaces.Discrete(self.n_actions)\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.state_shape, dtype=np.float32)\n","\n","        # Initialize the environment\n","        self.reset()\n","    \n","    def reset(self):\n","        '''\n","        Initalize temporal variables.\n","        '''\n","        # Intialize the values\n","        self.t = self.time_horizon\n","        self.positions_opened = []\n","        self.position_value = 0\n","        self.done = False\n","       \n","\n","        # Define some useful debug variables\n","        self.history_dates = []\n","        self.history_actions = []\n","        self.history_position_values = []\n","        self.history_profits = []\n","        self.history_rewards = []\n","        \n","        # Get first state\n","        # initial_state = self.data[:self.t]\n","        # initial_state = np.array(initial_state).reshape(1,self.time_horizon,self.n_features)\n","\n","        self.next_state = self.data[(self.t - self.time_horizon):self.t]\n","        self.next_state = np.array(self.next_state).reshape(1,self.time_horizon,self.n_features)\n","        \n","        return self.next_state\n","\n","    def step(self, action):\n","        # Compute position value and profits\n","        position_value = sum([self.data[self.t] for p in self.positions_opened])\n","        profits        = sum([self.data[self.t] - p for p in self.positions_opened])\n","        \n","        # Hold\n","        if action == 0:\n","            # Use the profits as reward\n","\n","            #VAL: Esto no sera? reward = 0\n","            #reward = profits\n","            reward = 0\n","        # Buy\n","        elif action == 1:\n","            # Open a new position\n","            self.positions_opened.append(self.data[self.t])\n","            \n","            # reward = -self.data[self.t-1]-self.comision #a.  Meter comisiones. Porcentaje comision\n","            reward = -self.data[self.t-1] #a.  Meter comisiones. Porcentaje comision\n","            reward = -(1-self.porcentaje_comision) #a.  Meter comisiones. Porcentaje comision\n","        # Sell\n","        elif action == 2:\n","            # Check that the agent as open positions:\n","            if len(self.positions_opened) > 0:\n","                # Close all opened positions\n","                self.positions_opened = []                \n","\n","                # Use the profits as reward\n","                # reward = profits\n","                # VAL: Esto será el valor de venta, no?\n","                # reward = self.data[self.t-1]-self.comision #a.  Meter comisiones. Porcentqaje comision\n","                reward = -self.data[self.t-1] #a.  Meter comisiones. Porcentaje comision\n","                reward = -(1-self.porcentaje_comision) #a.  Meter comisiones. Porcentaje comision\n","\n","            # else: # b.# - Aceptar inversion en corto.\n","                # Penalize selling when having no opened positions\n","            #     reward = -1 # b.# - Aceptar inversion en corto.\n","\n","        # Update debug variables\n","        #self.history_dates.append(self.data.index[self.t])\n","        self.history_actions.append(action)\n","        self.history_position_values.append(position_value)\n","        self.history_rewards.append(reward)\n","        \n","        # Prepare the next step\n","        self.t += self.time_skip\n","        self.next_state = self.data[(self.t - self.time_horizon):self.t]\n","        self.next_state = np.array(self.next_state).reshape(1,self.time_horizon,self.n_features)\n","\n","        print(self.t)\n","        # Check if we can continue operating\n","        if (self.t + self.time_skip) >= len(self.data):\n","            self.done = True\n","\n","        # Return (obs, reward, done, info)\n","        return (self.next_state, reward, self.done, {})\n","\n","    def render(self, mode = 'human', verbose = False):\n","        print('PROFITS: {:.3f}'.format(np.sum(self.history_rewards)))\n","        print('POSITION VALUE: {:.3f}'.format(self.position_value))\n","        plt.plot(self.next_state[0,:,0])\n","\n","# creado pro mi en ejercicio 6\n","class OneStockEnv_vender_porcion_porc_comis(gym.Env):\n","    '''\n","    Reinforcement Leaning environment representing a Stock Market with a single stock.\n","    '''\n","    def __init__(self, \n","                 data,\n","                 time_skip    = 1,\n","                 time_horizon = 10,\n","                 # comision = 0 #a.  Meter comisiones. Porcentaje comision\n","                 porcentaje_comision = 0.0007 #a.  Meter comisiones. Porcentaje comision\n","                 ):\n","        '''\n","        Initialization of the environment.\n","        \n","        Args:\n","            data\n","            time_skip    (int):      number of timesteps to skip between states\n","        '''\n","        # Get the data\n","        self.data = data\n","        self.time_horizon = time_horizon\n","        self.time_skip = time_skip\n","        #self.comision = comision #a.  Meter comisiones. Porcentaje comision\n","        self.porcentaje_comision = porcentaje_comision #a.  Meter comisiones. Porcentaje comision\n","        # Define actions\n","        self.actions = {0 : 'Hold', 1 : 'Buy', 2 : 'Sell'}\n","        \n","        # Save useful values\n","        self.n_timesteps = (self.data.shape[0] - self.time_horizon) \n","        self.n_features  = 1\n","        self.n_actions   = len(self.actions)\n","        self.state_shape = (self.time_horizon, self.n_features)\n","        \n","        # Define spaces\n","        self.action_space      = spaces.Discrete(self.n_actions)\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.state_shape, dtype=np.float32)\n","\n","        # Initialize the environment\n","        self.reset()\n","        \n","        \n","        \n","    \n","    def reset(self):\n","        '''\n","        Initalize temporal variables.\n","        '''\n","        # Intialize the values\n","        self.t = self.time_horizon\n","        self.positions_opened = []\n","        \n","        self.position_value = 0\n","        self.done = False\n","        \n","            \n","\n","        # Define some useful debug variables\n","        self.history_dates = []\n","        self.history_actions = []\n","        self.history_position_values = []\n","        self.history_profits = []\n","        self.history_rewards = []\n","        \n","        # Get first state\n","        # initial_state = self.data[:self.t]\n","        # initial_state = np.array(initial_state).reshape(1,self.time_horizon,self.n_features)\n","\n","        self.next_state = self.data[(self.t - self.time_horizon):self.t]\n","        self.next_state = np.array(self.next_state).reshape(1,self.time_horizon,self.n_features)\n","        \n","        return self.next_state\n","\n","    def step(self, action,porcion): # porcion de venta\n","        # Compute position value and profits\n","        position_value = sum([self.data[self.t] for p in self.positions_opened])\n","        profits        = sum([self.data[self.t] - p for p in self.positions_opened])\n","\n","        # Hold\n","        if action == 0:\n","            # Use the profits as reward\n","\n","            #VAL: Esto no sera? reward = 0\n","            #reward = profits\n","            reward = 0\n","        # Buy\n","        elif action == 1: \n","            \n","            \n","\n","            # c. si compro afecto un 0.01% el valor del precio de compra hacia arriba( por el procentaje de lo que voy a comprar)\n","            print(f'precio de compra original:{self.data[self.t]}') #nuevo. c- Modificar el precio de salida teniendo en cuenta el impacto\n","            self.data[self.t] = self.data[self.t]*porcion*(1+0.0001) #nuevo c- Modificar el precio de salida teniendo en cuenta el impacto\n","            print(f'precio de compra afectado final:{self.data[self.t]}') #nuevo c- Modificar el precio de salida teniendo en cuenta el impacto       \n","           \n","            # Open a new position\n","            self.positions_opened.append(self.data[self.t]) # compro una proporcion que pueden ser 2 acioens o 3, etc           \n","            # reward = -self.data[self.t-1]-self.comision #a.  Meter comisiones. Porcentaje comision\n","            reward = -self.data[self.t-1] * porcion #a.  Meter comisiones. Porcentaje comision # compro una proporcion que pueden ser 2 acciones o 3, etc\n","            reward = reward*(1-self.porcentaje_comision) #a.  Meter comisiones. Porcentaje comision\n","            # en_open_o_en_close = 1\n","            \n","        # Sell\n","        elif action == 2:\n","            # Check that the agent as open positions:\n","            if len(self.positions_opened) > 0:\n","                # Close all opened positions\n","                a_vender = sum(self.positions_opened) # VENDER PORCION\n","                self.positions_opened = []\n","               \n","                # c. si vendo afecto un 0.01% el valor del precio de venta hacia abajo( por el porcentaje de lo que voy a comprar)\n","                # pero en este caso entiendo que es t-1\n","                print(f'precio de venta original:{self.data[self.t-1]}') #nuevo\n","                self.data[self.t-1] = self.data[self.t-1]*porcion*(1-0.0001) #nuevo\n","                print(f'precio de venta afectado final:{self.data[self.t-1]}') #nuevo                \n","               \n","               \n","               \n","                # Use the profits as reward\n","                # reward = profits\n","                # reward = self.data[self.t-1]-self.comision #a.  Meter comisiones. Porcentqaje comision # porcentaje de lo que vendo\n","                reward = -self.data[self.t-1]*porcion #a.  Meter comisiones. Porcentaje comision\n","                reward = reward*(1-self.porcentaje_comision) #a.  Meter comisiones. Porcentaje comision\n","               \n","                # print(-self.data[self.t-1]*(1-porcion))\n","               \n","                # print(reward*(1-porcion))\n","                print(f'total de venta: {a_vender}')\n","                porcion_no_vendida = a_vender*(1-porcion)# me quedo con la porcion que no he vendido\n","                print(f'vendido: {a_vender*porcion}')\n","                self.positions_opened.append(porcion_no_vendida)\n","                \n","          \n","                \n","                \n","                \n","\n","            # else: # b.# - Aceptar inversion en corto.\n","            #    # Penalize selling when having no opened positions\n","            #    reward = -1 # b.# - Aceptar inversion en corto.\n","\n","        # Update debug variables\n","        #self.history_dates.append(self.data.index[self.t])\n","        self.history_actions.append(action)\n","        self.history_position_values.append(position_value)\n","        self.history_rewards.append(reward)\n","        \n","        # Prepare the next step\n","        self.t += self.time_skip\n","        self.next_state = self.data[(self.t - self.time_horizon):self.t]\n","        self.next_state = np.array(self.next_state).reshape(1,self.time_horizon,self.n_features)\n","\n","        print(self.t)\n","        # Check if we can continue operating\n","        if (self.t + self.time_skip) >= len(self.data):\n","            self.done = True\n","        \n","        \n","        # print(self.next_state) ############\n","        # Return (obs, reward, done, info)\n","        return (self.next_state, reward, self.done, {})\n","\n","    def render(self, mode = 'human', verbose = False):\n","        print('PROFITS: {:.3f}'.format(np.sum(self.history_rewards)))\n","        print('POSITION VALUE: {:.3f}'.format(self.position_value))\n","        plt.plot(self.next_state[0,:,0])\n","        \n","# Create environment\n","env = OneStockEnv(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","# env = OneStockEnv_vender_porcion_porc_comis(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","\n","# env.reset()\n","# plt.imshow(env.render()[0])\n","# plt.axis('off');"]},{"cell_type":"code","execution_count":10,"id":"FZ3e2NgGmYN-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":855,"status":"ok","timestamp":1684217805386,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"FZ3e2NgGmYN-","outputId":"2c5cbbb3-d3c8-4f9a-c9f9-93a386df46a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(20, 1)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["(20, 1)"]},"metadata":{},"execution_count":10}],"source":["env = OneStockEnv(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","print(env.observation_space.shape)\n","env = OneStockEnv_vender_porcion_porc_comis(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","env.observation_space.low, env.observation_space.high, env.observation_space.shape, env.observation_space.dtype, \n","env.observation_space.shape\n","\n","        # Define spaces\n","        #self.action_space      = spaces.Discrete(self.n_actions)\n","        #self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.state_shape, dtype=np.float32)"]},{"cell_type":"code","execution_count":11,"id":"afaUEgdluu9z","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":40344,"status":"error","timestamp":1684217870171,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"afaUEgdluu9z","outputId":"9cd824e5-9db7-4e49-dd75-95ec753016f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/carlosluis/stable-baselines3@fix_tests\n","  Cloning https://github.com/carlosluis/stable-baselines3 (to revision fix_tests) to /tmp/pip-req-build-tfi25_0e\n","  Running command git clone --filter=blob:none --quiet https://github.com/carlosluis/stable-baselines3 /tmp/pip-req-build-tfi25_0e\n","  Running command git checkout -b fix_tests --track origin/fix_tests\n","  Switched to a new branch 'fix_tests'\n","  Branch 'fix_tests' set up to track remote branch 'fix_tests' from 'origin'.\n","  Resolved https://github.com/carlosluis/stable-baselines3 to commit 6617e6e73cb3a70f3e88cea780ea12bed95c099e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting gym==0.26.2 (from stable-baselines3==2.0.0a0)\n","  Downloading gym-0.26.2.tar.gz (721 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (1.22.4)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (2.0.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (3.7.1)\n","Collecting importlib-metadata~=4.13 (from stable-baselines3==2.0.0a0)\n","  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2->stable-baselines3==2.0.0a0) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata~=4.13->stable-baselines3==2.0.0a0) (3.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3==2.0.0a0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3==2.0.0a0) (16.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a0) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a0) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a0) (1.3.0)\n","Building wheels for collected packages: stable-baselines3, gym\n","  Building wheel for stable-baselines3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stable-baselines3: filename=stable_baselines3-2.0.0a0-py3-none-any.whl size=174565 sha256=8e4b23a10eafd892ecf19672403779d14d46f87aad4b9d747350f50cae87acd3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-nnc6h61n/wheels/cc/40/56/e6db2f8ff9427b0849f4c6ddbe003a53a5f61f31aae2f9ccb7\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827637 sha256=75f483bc916e3410a58cec47233880f8cb083db48a74a27a05297cbbb5be5af5\n","  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n","Successfully built stable-baselines3 gym\n","Installing collected packages: importlib-metadata, gym, stable-baselines3\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-0.26.2 importlib-metadata-4.13.0 stable-baselines3-2.0.0a0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gym"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym\n","  Using cached gym-0.26.2-py3-none-any.whl\n","Collecting numpy>=1.18.0 (from gym)\n","  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle>=1.2.0 (from gym)\n","  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n","Collecting gym-notices>=0.0.4 (from gym)\n","  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n","Installing collected packages: gym-notices, numpy, cloudpickle, gym\n","  Attempting uninstall: gym-notices\n","    Found existing installation: gym-notices 0.0.8\n","    Uninstalling gym-notices-0.0.8:\n","      Successfully uninstalled gym-notices-0.0.8\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.26.2\n","    Uninstalling gym-0.26.2:\n","      Successfully uninstalled gym-0.26.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-2.2.1 gym-0.26.2 gym-notices-0.0.8 numpy-1.24.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cloudpickle","gym","gym_notices","numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.0.0a0)\n","Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.26.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.24.3)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.0.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (4.13.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2->stable_baselines3) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (16.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable_baselines3) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable_baselines3) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Collecting ale-py~=0.8.0 (from gym[accept-rom-license,atari])\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license,atari])\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (5.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=50257b5f4499498811854996f3c7ae156739f1e9fb917b24af6ca827eecc2a6e\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: ale-py, AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n","  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"]},{"output_type":"error","ename":"NameNotFound","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ef4f5da55c26>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# ENV_NAME = 'SpaceInvaders-v5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mENV_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Asterix-v5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m             ):\n\u001b[1;32m    606\u001b[0m                 logger.warn(\n\u001b[0;32m--> 607\u001b[0;31m                     \u001b[0;34m\"You are trying to use 'human' rendering for an environment that doesn't natively support it. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m                     \u001b[0;34m\"The HumanRendering wrapper is being applied to your environment.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mdefault_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspec_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_specs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdefault_spec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" It provides the default version {default_spec[0].id}`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m \u001b[0mbut\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mVersionNotFound\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mused\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEnvironment\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameNotFound\u001b[0m: Environment Asterix doesn't exist. "]}],"source":["# cargo para analizar\n","# RL3_Stablebaselines3_SpaceInvaders.ipynb\n","# Esto hacía falta antes\n","#!pip install --upgrade --force-reinstall gym\n","\n","# La opcion oficial no funciona 5/4/2023\n","#!pip install stable_baselines3[extra]\n","\n","# Alternativa \n","!pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests\n","!pip install --upgrade --force-reinstall gym\n","!pip install stable_baselines3\n","!pip3 install gym[atari,accept-rom-license]\n","import gym\n","#from gym.wrappers import Monitor\n","import base64\n","from pathlib import Path\n","from IPython import display as ipythondisplay\n","from stable_baselines3 import A2C\n","from stable_baselines3.common.env_util import make_vec_env\n","import os\n","\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","# Cargamos el entorno\n","# ENV_NAME = 'SpaceInvaders-v5'\n","ENV_NAME = 'Asterix-v5'\n","env = gym.make(ENV_NAME)\n","height, width, channels = env.observation_space.shape\n","actions = env.action_space.n\n","\n","model = A2C(\"MlpPolicy\", env, verbose=1) #Podemos elegir entre varias politicas, como MLPpolicy, CnnPolicy, MultiInputPolicy\n","model.learn(total_timesteps=25000)\n","model.save(\"a2c_Asterix\")\n","\n","recompensa_media, desviacion = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=20)\n","print(f\"recompensa_media:{recompensa_media:.2f} +/- {desviacion:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"89ciBsha5cHb","metadata":{"id":"89ciBsha5cHb"},"outputs":[],"source":["# Instalar librerías\n","try:\n","    from google.colab import drive\n","    %tensorflow_version 2.x\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False\n","\n","if COLAB:\n","  !sudo apt-get install -y xvfb ffmpeg\n","  !pip install -q 'gym==0.10.11'\n","  !pip install -q 'imageio==2.4.0'\n","  !pip install -q PILLOW\n","  !pip install -q 'pyglet==1.3.2'\n","  !pip install -q pyvirtualdisplay\n","  !pip install -q --upgrade tensorflow-probability\n","  !pip install -q tf-agents\n","\n","\n","# Roms de Atari ya que Gym no provee las Roms\n","! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/\n","\n","!pip install ale-py"]},{"cell_type":"code","execution_count":null,"id":"jT4VL_jPuvPC","metadata":{"id":"jT4VL_jPuvPC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"EFuSY_fsuvcE","metadata":{"id":"EFuSY_fsuvcE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"SmmvEZhZpUdm","metadata":{"id":"SmmvEZhZpUdm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"id":"9c790e17-ca48-424a-9af1-3292e42ce67d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":572,"status":"error","timestamp":1684217878412,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"9c790e17-ca48-424a-9af1-3292e42ce67d","outputId":"dbbbfe0a-5df9-4ece-e412-e3329e54393d"},"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-96e025b7f54e>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0;31m# Choose an action by using epsilon greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"]}],"source":["# 2 Busco unos preoptimos valores de discount_factor, alpha, y epsilon como hice en el ej 4\n","\n","# igual al de eljercicio 4\n","\n","# Install pygame and virtual display (needed in Colab)\n","# !pip install pygame\n","# !pip install pyvirtualdisplay\n","# !apt install xvfb\n","# from pyvirtualdisplay import Display\n","# Display(\"xvfb\").start()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gym\n","from tqdm.notebook import tqdm, trange\n","# To build and display animations\n","# from PIL import Image\n","# from IPython import display\n","\n","\n","num_episodes = 2000\n","rList = []\n","learning_rate_list = [0.5,0.7,0.9]\n","gamma_list = [.92,.94,.96,.98]\n","eps_list = [.08,.095,.105,.12]\n","\n","import pandas as pd\n","resultados = pd.DataFrame(columns=['learning_rate','gamma','eps','sum(rList)/num_episodes','episode+1'])\n","\n","for learning_rate in learning_rate_list: \n","    for gamma in gamma_list:\n","        for eps in eps_list:\n","            # Initialize table with all zeros\n","            # Q = np.zeros([env.observation_space.n, env.action_space.n])\n","            ###############env.observation_space.shape[0] 20\n","            Q = np.zeros([env.observation_space.shape[0], env.action_space.n]) # nuevo\n","            # print(Q)\n","            # Set learning parameters\n","            nA = env.action_space.n\n","\n","            # create list to contain total rewards\n","            rList = []\n","            episode = 0\n","            while sum(rList)/num_episodes < 0.05:\n","                episode += 1\n","                num_episodes = episode\n","\n","                # Reset environment and get first new observation\n","                \n","                # ME SALE ESTE ERROR DE LA s IndexError: arrays used as indices must be of integer (or boolean) type en ai = np.argmax(Q[s,:])\n","                s = env.reset()\n","                # entonces me he creado esto\n","                # s = (env.reset()*1000).astype(int)\n","\n","\n","\n","                rAll = 0\n","                # The Q-Table learning algorithm\n","                for _ in range(100):\n","                    # Choose an action by using epsilon greedy \n","                    ai = np.argmax(Q[s,:])\n","                    \n","                    p = np.ones((nA,))*eps/(nA-1)\n","                    p[ai] = 1-eps\n","                    a  = np.random.choice(np.arange(len(p)), p=p)\n","                    # Get new state and reward from environment\n","                    s1, reward, terminated, truncated, info = env.step(a)\n","                    # Update Q-Table with new knowledge\n","                    Q[s,a] = Q[s,a] + learning_rate * (reward + gamma * np.max(Q[s1,:]) - Q[s,a])\n","                    \n","                    rAll += reward\n","                    s = s1\n","                    if terminated or truncated:\n","                        break\n","                # break\n","                rList.append(rAll)\n","\n","                if (episode+1) % 10 == 0:\n","                    print(\"Episode %4d, reward %.2f\" % (episode+1, sum(rList)/num_episodes))\n","\n","            print(\"Score over time:\", sum(rList)/num_episodes)\n","            print(\"Last reward:\", rList[-1])\n","\n","            plt.plot(rList)\n","\n","\n","            #nuevo\n","            resultados.loc[len(resultados .index)] = [learning_rate,gamma,eps,sum(rList)/num_episodes,episode+1] \n","\n","\n","            print(learning_rate,gamma,eps,sum(rList)/num_episodes,episode+1)\n","resultados"]},{"cell_type":"code","execution_count":15,"id":"AhqRJXiLrv5K","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1684217959503,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"AhqRJXiLrv5K","outputId":"88d7cca6-11e3-4c7a-980c-1829c57e6551"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(3)"]},"metadata":{},"execution_count":15}],"source":["env.action_space"]},{"cell_type":"code","execution_count":16,"id":"7864d982-f5a2-4d1d-a542-bc2b280f5332","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684217961466,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"7864d982-f5a2-4d1d-a542-bc2b280f5332","outputId":"c05f5e6d-0ac9-463c-e695-9a3ceb9962b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'reinforcement-learning' already exists and is not an empty directory.\n"]}],"source":["# 3\n","# si busco discount_factor, alpha, y epsilon como se dice en el\n","# notebook RL_Q_learning_paso_a_paso ya que ahi no se necesita\n","# env.observation_space.n\n","\n","!git clone https://github.com/dennybritz/reinforcement-learning.git\n","import numpy as np\n","import pprint\n","import sys\n","sys.path.append(\"/content/reinforcement-learning/\")\n","from lib.envs.gridworld import GridworldEnv\n","from collections import defaultdict\n","import itertools\n","\n","# Leer el entorno\n","pp = pprint.PrettyPrinter(indent=2)\n","# env = GridworldEnv()\n","\n","# Definir funcion auxiliar\n","def make_epsilon_greedy_policy(Q, epsilon, nA):\n","    \"\"\"\n","    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n","    Args:\n","    Q: A dictionary that maps from state -> action-values.\n","    Each value is a numpy array of length nA (see below)\n","    epsilon: The probability to select a random action . float between 0 and 1.\n","    nA: Number of actions in the environment.\n","    Returns:\n","    A function that takes the observation as an argument and returns\n","    the probabilities for each action in the form of a numpy array of length nA.\n","    \"\"\"\n","    def policy_fn(observation):\n","        print(f'observation{observation}')\n","        A = np.ones(nA, dtype=float) * epsilon / (nA-1)\n","        best_action = np.argmax(Q[observation])\n","        A[best_action] = (1.0 - epsilon)\n","        return A\n","    return policy_fn\n","\n","# Q-LEARNING\n","def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n","    \"\"\"\n","    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n","    while following an epsilon-greedy policy\n","    Args:\n","    env: OpenAI environment.\n","    num_episodes: Number of episodes to run for.\n","    discount_factor: Gamma discount factor.\n","    alpha: TD learning rate.\n","    epsilon: Chance the sample a random action. Float betwen 0 and 1.\n","    Returns:\n","    A tuple (Q, episode_lengths).\n","    Q is the optimal action-value function, a dictionary mapping state -> action values\n","    stats is an EpisodeStats object with two numpy arrays for episode_lengths and episo\n","    \"\"\"\n","    # The final action-value function.\n","    # A nested dictionary that maps state -> (action -> action-value).\n","    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n","    \n","    # Keeps track of useful statistics\n","    episode_lengths=np.zeros(num_episodes)\n","    episode_rewards=np.zeros(num_episodes)\n","    # The policy we're following\n","    \n","    \n","    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n","    for i_episode in range(num_episodes):\n","        # Print out which episode we're on, useful for debugging.\n","        if (i_episode + 1) % 100 == 0:\n","            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n","            sys.stdout.flush()\n","        # Reset the environment and pick the first action\n","        state = env.reset()\n","        print(f'state {state}')###########################\n","        # One step in the environment\n","        # total_reward = 0.0\n","        for t in itertools.count():\n","            # Take a step\n","            action_probs = policy(state)\n","            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n","            next_state, reward, done, _ = env.step(action)\n","            # Update statistics\n","            episode_rewards[i_episode] += reward\n","            episode_lengths[i_episode] = t\n","            # TD Update\n","            best_next_action = np.argmax(Q[next_state])\n","            td_target = reward + discount_factor * Q[next_state][best_next_action]\n","            td_delta = td_target - Q[state][action]\n","            Q[state][action] += alpha * td_delta\n","            print(f'Q{Q}')###########################\n","            if done:\n","                break\n","            state = next_state\n","    return Q, episode_lengths, episode_rewards\n","# Paso a paso\n","#Parametros\n","discount_factor=1.0\n","alpha=0.5\n","epsilon=0.1\n","\n","#Inicializamos\n","Q = defaultdict(lambda: np.zeros(env.action_space.n))\n","# The policy we're following\n","policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n","# Reset the environment and pick the first action\n","state = env.reset()\n","t = 0\n","\n"]},{"cell_type":"code","execution_count":17,"id":"IYn7zuOSMQSS","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":250,"status":"error","timestamp":1684217967198,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"IYn7zuOSMQSS","outputId":"468935d9-ad2b-4582-d5c3-8283c10b4102"},"outputs":[{"output_type":"stream","name":"stdout","text":["state [[[50.   ]\n","  [50.505]\n","  [55.375]\n","  [55.62 ]\n","  [52.48 ]\n","  [52.475]\n","  [54.05 ]\n","  [52.64 ]\n","  [51.15 ]\n","  [51.35 ]\n","  [49.595]\n","  [50.475]\n","  [50.505]\n","  [50.37 ]\n","  [50.37 ]\n","  [50.8  ]\n","  [53.315]\n","  [53.725]\n","  [55.28 ]\n","  [56.17 ]]]\n","observation[[[50.   ]\n","  [50.505]\n","  [55.375]\n","  [55.62 ]\n","  [52.48 ]\n","  [52.475]\n","  [54.05 ]\n","  [52.64 ]\n","  [51.15 ]\n","  [51.35 ]\n","  [49.595]\n","  [50.475]\n","  [50.505]\n","  [50.37 ]\n","  [50.37 ]\n","  [50.8  ]\n","  [53.315]\n","  [53.725]\n","  [55.28 ]\n","  [56.17 ]]]\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-597e3fb37db1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_episode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_episode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-417344c10160>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, num_episodes, discount_factor, alpha, epsilon)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Take a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-417344c10160>\u001b[0m in \u001b[0;36mpolicy_fn\u001b[0;34m(observation)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'observation{observation}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"]}],"source":["# 4\n","env = OneStockEnv(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","#env = GridworldEnv()\n","# env = OneStockEnv_vender_porcion_porc_comis(data = data.values,time_horizon = 20,porcentaje_comision=0.0007)\n","\n","\n","Q, stats_episode_lengths, stats_episode_rewards = q_learning(env, 200)\n","import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(stats_episode_lengths)\n","plt.figure()\n","plt.plot(stats_episode_rewards)"]},{"cell_type":"code","execution_count":18,"id":"hCzucaZvMQgx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1684217977681,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"hCzucaZvMQgx","outputId":"769f0493-046f-4b49-ab0b-9707b98e5dda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(<function __main__.<lambda>()>, {})"]},"metadata":{},"execution_count":18}],"source":["# Action State\n","Q"]},{"cell_type":"code","execution_count":null,"id":"D91xS-GHNFYv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1683697803260,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"D91xS-GHNFYv","outputId":"8d02b7a2-7fe6-4cd1-a664-7c17bb9b228e"},"outputs":[{"name":"stdout","output_type":"stream","text":["T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","-2.0\n"]}],"source":["# 5\n","# Vamos a ver que ha aprendido!\n","done = False\n","G, reward = 0,0\n","state = env.reset()\n","while done != True:\n","        action = np.argmax(Q[state]) #1\n","        state2, reward, done, info = env.step(action) #2\n","        #Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n","        G += reward\n","        env._render()\n","        state = state2   \n","print(G)"]},{"cell_type":"code","execution_count":null,"id":"H_UnwtZHNFlu","metadata":{"id":"H_UnwtZHNFlu"},"outputs":[],"source":["p = np.zeros(16,)\n","for s in np.arange(0,16):\n","    p[s] = np.argmax(Q[s][:])"]},{"cell_type":"code","execution_count":null,"id":"z44Mt0fMNFxl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":582,"status":"ok","timestamp":1683697811172,"user":{"displayName":"Fernando Gallego Romualdo","userId":"05683736919191850259"},"user_tz":-180},"id":"z44Mt0fMNFxl","outputId":"ba546c8e-e652-413b-ff90-c597e4700fa9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 3. 3. 2.]\n"," [0. 3. 2. 2.]\n"," [0. 2. 2. 2.]\n"," [0. 1. 1. 0.]]\n"]}],"source":["print(np.resize(p,(4,4)))"]},{"cell_type":"code","execution_count":null,"id":"zRR5No07NF-X","metadata":{"id":"zRR5No07NF-X"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"SfMII9XsMQtJ","metadata":{"id":"SfMII9XsMQtJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"FVRxoK6MMQ5B","metadata":{"id":"FVRxoK6MMQ5B"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"VOshIbm3MRFJ","metadata":{"id":"VOshIbm3MRFJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6ce5c373-5e8d-4ce5-b590-f8561b959b5c","metadata":{"id":"6ce5c373-5e8d-4ce5-b590-f8561b959b5c"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}